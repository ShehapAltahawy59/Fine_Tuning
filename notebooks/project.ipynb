{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from openai import OpenAI\n",
    "from markdownify import markdownify as md\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from autogen_core.models import ModelInfo\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "load_dotenv()\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_agentchat.messages import MultiModalMessage\n",
    "from pydantic import BaseModel\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from typing import List\n",
    "gemini_api_key = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_folder(folder_path):\n",
    "    \n",
    "    full_text =\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, file_name)\n",
    "            print(f\"ðŸ“„ Processing: {file_name}\")\n",
    "            \n",
    "            doc = fitz.open(pdf_path)\n",
    "            full_text += \"\\n\".join(page.get_text() for page in doc)\n",
    "            print(f\"âœ… Extracted from {file_name}: {len(full_text)} characters.\")\n",
    "            \n",
    "            \n",
    "    \n",
    "    return full_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Merge broken lines\n",
    "    text = re.sub(r'\\n(?=[a-z])', ' ', text)  # join words split by line breaks\n",
    "    # Remove multiple newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    return text.strip()\n",
    "    \n",
    "# Step 2: Chunk text into sections\n",
    "def chunk_text(text, min_words=200, max_words=400):\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    chunks = []\n",
    "    for p in paragraphs:\n",
    "        words = p.split()\n",
    "        if min_words <= len(words) <= max_words:\n",
    "            chunks.append(p.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_qa_from_chunk(chunk, agent):\n",
    "    text_message = TextMessage(content=chunk, source=\"User\")\n",
    "    result = await agent.on_messages(\n",
    "            [text_message], \n",
    "            cancellation_token=CancellationToken()\n",
    "        )\n",
    "    response = result.chat_message.content\n",
    "    response = response.model_dump()\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def build_dataset(chunks, client):\n",
    "    dataset = []\n",
    "    for chunk in chunks:\n",
    "        #try:\n",
    "            qas = await generate_qa_from_chunk(chunk, client)\n",
    "            qas = qas[\"output\"]\n",
    "            \n",
    "            \n",
    "            for qa in qas:\n",
    "                if isinstance(qa, str):\n",
    "                    qas_parsed = json.loads(qa)\n",
    "                else:\n",
    "                    qas_parsed = qa\n",
    "                dataset.append({\n",
    "                    \"context\": chunk,\n",
    "                    \"question\": qas_parsed[\"question\"],\n",
    "                    \"answer\": qas_parsed[\"answer\"]\n",
    "                })\n",
    "                await asyncio.sleep(10)\n",
    "        # except Exception as e:\n",
    "        #     print(\"Error processing chunk:\", e)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    n_total = len(data)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    \n",
    "    train_data = data[:n_train]\n",
    "    val_data = data[n_train:n_train + n_val]\n",
    "    test_data = data[n_train + n_val:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Step 6: Save to JSONL\n",
    "def save_dataset(train, val, test,prefix=\"egypt_pdf_qa\"):\n",
    "    with open(f\"{prefix}_train.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in train:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    with open(f\"{prefix}_val.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in val:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    with open(f\"{prefix}_test.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in test:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"âœ… Saved {len(train)} train and {len(val)} validation samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_client = OpenAIChatCompletionClient(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        api_key=gemini_api_key,\n",
    "        model_info=ModelInfo(vision=True, function_calling=True, json_output=True, family=\"unknown\", structured_output=True)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qustion(BaseModel):\n",
    "    question: str\n",
    "    answer : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class output(BaseModel):\n",
    "    output : List[Qustion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_Specialist = AssistantAgent(\n",
    "    name=\"history_Speciallist_agent\",\n",
    "    model_client=gemini_client,\n",
    "    system_message=\"\"\"You are a history expert assistant. From the paragraph , generate 2â€“3 factual questionâ€“answer pairs.\n",
    "    -only make qustions relate to history.\n",
    "    \"\"\"\n",
    "    ,\n",
    "    output_content_type=output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing: 05. A Short History of Egypt â€“ to About 1970 author Aerospace Computing Lab.pdf\n",
      "âœ… Extracted from 05. A Short History of Egypt â€“ to About 1970 author Aerospace Computing Lab.pdf: 149972 characters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_path = \"pdf_data\"  # Path to your local PDF\n",
    "\n",
    "text = extract_text_from_folder(pdf_path)\n",
    "text = clean_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Extracted 78 chunks from PDF\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,  # characters or tokens approx\n",
    "    chunk_overlap=500\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "print(f\"ðŸ“„ Extracted {len(chunks)} chunks from PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Women  had rights  that many of their  ancient counterparts  didnâ€™t, including the  right to own property  and demand  divorce\\nScientists were unable to confirm  they had found Hatshepsutâ€™s  mummy until they identified one  of her teeth from a separate find\\nThe Mortuary Temple of Hatshepsut  is located on the west bank of the \\nNile, in the Valley of the Kings\\n67\\nHer stepson Thutmose III went on to rule  for a further 30 years, proving to be a similarly  ambitious builder and a mighty warrior. He led 17  campaigns in enemy-held territory, and conquered  land as far north as Syria and as far south as the \\nFourth Cataract of the Nile. Meanwhile, the relics  of Hatshepsutâ€™s reign continued to stand proud on  the Egyptian skyline, her towering obelisks and  imposing statues casting a shadow in her memory  upon the land she once called hers. \\nHowever, towards the end of Thutmoseâ€™s regency,  he ordered that his stepmotherâ€™s cartouches and  images be chiselled away, and her statues torn  down, disfigured and smashed before being buried  in a pit. There was even an attempt at Karnak to  surround her obelisks with walls. Various theories  have been given to explain this sudden and  dramatic turn of events. Some argue that this was  carried out as a typical act of self-promotion during \\nThutmoseâ€™s waning years, while others suggest  it was simply a money-saving method whereby  existing buildings could be accredited to the  current king. \\nIt has been suggested that when \\nThutmose came of age, he  demoted Hatshepsut back to the  role of regent, and attempted to  eliminate any evidence of her  as pharaoh to claim that the  royal succession ran directly to  him from his father. It seems  the most likely explanation is  not a sinister one, but rather a  cold, rational attempt to extinguish  the memory of an â€œunconventional  female king whose reign might possibly  be interpreted by future generations as a grave  offence against maâ€™at,â€ as Tyldesley put it. She  proposes that Thutmose carefully considered how  the successful reign of a female pharaoh might  affect the Egyptian social order, and eventually  made the decision to eliminate her records so as  to prevent a feminist uprising. Hatshepsutâ€™s crime,  therefore, may be nothing more than the fact she  was a woman.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks 160 to 170...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 160 to 170\n",
      "\n",
      "Processing chunks 170 to 180...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 170 to 180\n",
      "\n",
      "Processing chunks 180 to 190...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 180 to 190\n",
      "\n",
      "Processing chunks 190 to 200...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 190 to 200\n",
      "\n",
      "Processing chunks 200 to 210...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 200 to 210\n",
      "\n",
      "Processing chunks 210 to 220...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 210 to 220\n",
      "\n",
      "Processing chunks 220 to 230...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 220 to 230\n",
      "\n",
      "Processing chunks 230 to 240...\n",
      "âœ… Saved 21 train and 4 validation samples.\n",
      "âœ… Saved dataset for chunks 230 to 240\n",
      "\n",
      "Processing chunks 240 to 250...\n",
      "âœ… Saved 8 train and 1 validation samples.\n",
      "âœ… Saved dataset for chunks 240 to 250\n"
     ]
    }
   ],
   "source": [
    "async def process_in_batches(chunks, client, batch_size=50):\n",
    "    for start in range(160, len(chunks), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_chunks = chunks[start:end]\n",
    "        \n",
    "        print(f\"\\nProcessing chunks {start} to {end}...\")\n",
    "        dataset = await build_dataset(batch_chunks, client)\n",
    "        \n",
    "        train, val, test = split_dataset(dataset)\n",
    "        save_dataset(train, val, test)\n",
    "        \n",
    "        print(f\"âœ… Saved dataset for chunks {start} to {end}\")\n",
    "\n",
    "# Example run\n",
    "await process_in_batches(chunks, history_Specialist,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
