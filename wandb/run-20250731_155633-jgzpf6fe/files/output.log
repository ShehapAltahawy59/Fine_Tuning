  0%|                                                                                       | 0/1431 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Traceback (most recent call last):
  File "d:\New folder (10)\Fine_Tuning\main.py", line 52, in <module>
    training_pipeline.run()
  File "d:\New folder (10)\Fine_Tuning\training_evaluation_pipeline\training_pipeline.py", line 219, in run
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2207, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2608, in _inner_training_loop
    self.optimizer.step()
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\accelerate\optimizer.py", line 179, in step
    self.optimizer.step(closure)
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\optim\lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\optim\optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\bitsandbytes\optim\optimizer.py", line 288, in step
    self.init_state(group, p, gindex, pindex)
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\bitsandbytes\optim\optimizer.py", line 470, in init_state
    state["state1"] = self.get_state_buffer(p, dtype=torch.uint8)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\bitsandbytes\optim\optimizer.py", line 331, in get_state_buffer
    buff = F.get_paged(*p.shape, dtype=dtype, device=p.device)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\bitsandbytes\functional.py", line 204, in get_paged
    cuda_ptr = lib.cget_managed_ptr(ct.c_size_t(num_bytes))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\mosta\AppData\Local\Programs\Python\Python312\Lib\site-packages\bitsandbytes\cextension.py", line 55, in throw_on_call
    raise RuntimeError(
RuntimeError: Method 'cget_managed_ptr' not available in CPU-only version of bitsandbytes.
Reinstall with GPU support or use CUDA-enabled hardware.
